import joblib
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import ttest_ind

#  Load dataset
MERGED_DATA_PATH = "data/merged/cleaned_merged_data2.csv"

if not os.path.exists(MERGED_DATA_PATH):
    raise FileNotFoundError("‚ùå Cleaned dataset not found! Run `data_merging.py` first.")

df = pd.read_csv(MERGED_DATA_PATH)

#  Select features & target (Updated with constructor features)
selected_features = ["grid", "driver_experience", "avg_team_points", "laps", 
                     "year", "constructor_standings", "constructor_points"]
target = "positionOrder"

#  Ensure all selected features exist
missing_features = [col for col in selected_features if col not in df.columns]
if missing_features:
    raise KeyError(f"‚ùå Missing Features in Dataset: {missing_features}")

#  Prepare data
X = df[selected_features]
y = df[target]

#  Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#  Handle missing values before scaling
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)

#  Standardize Numerical Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#  Save Scaler
joblib.dump(scaler, "models/scaler.pkl")

# ------------------- üîπ Random Forest Hyperparameter Tuning üîπ -------------------
rf_param_grid = {
    "n_estimators": [50, 100, 200, 300],
    "max_depth": [10, 20, 30, None],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
}

rf_model = RandomForestRegressor(random_state=42)
rf_search = RandomizedSearchCV(rf_model, rf_param_grid, n_iter=10, cv=3, n_jobs=-1, random_state=42)
rf_search.fit(X_train_scaled, y_train)

best_rf_model = rf_search.best_estimator_
print("\n Best Random Forest Hyperparameters:", rf_search.best_params_)

# ------------------- üîπ XGBoost Hyperparameter Tuning üîπ -------------------
xgb_param_grid = {
    "n_estimators": [50, 100, 200, 300],
    "max_depth": [3, 6, 9, 12],
    "learning_rate": [0.01, 0.05, 0.1, 0.2],
    "subsample": [0.6, 0.8, 1.0],
    "colsample_bytree": [0.6, 0.8, 1.0],
}

xgb_model = XGBRegressor(random_state=42)
xgb_search = RandomizedSearchCV(xgb_model, xgb_param_grid, n_iter=10, cv=3, n_jobs=-1, random_state=42)
xgb_search.fit(X_train_scaled, y_train)

best_xgb_model = xgb_search.best_estimator_
print("\n Best XGBoost Hyperparameters:", xgb_search.best_params_)

#  Evaluate Models with Best Parameters
def evaluate_model(model, X_test_scaled, y_test, model_name):
    preds = model.predict(X_test_scaled)
    mae = mean_absolute_error(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)

    print(f"\nüèéÔ∏è {model_name} Performance:")
    print(f"   - MAE: {mae:.4f}")
    print(f"   - RMSE: {rmse:.4f}")
    print(f"   - R¬≤ Score: {r2:.4f}")

    return preds

rf_preds = evaluate_model(best_rf_model, X_test_scaled, y_test, "Random Forest (Tuned)")
xgb_preds = evaluate_model(best_xgb_model, X_test_scaled, y_test, "XGBoost (Tuned)")

#  Conduct T-Test Between Models
print("\nüìä Conducting T-Test Between Models:")
t_stat, p_value = ttest_ind(rf_preds, xgb_preds)

print(f"\nüîπ Random Forest vs XGBoost")
print(f"   - T-Statistic: {t_stat:.4f}")
print(f"   - P-Value: {p_value:.6f}")

if p_value < 0.05:
    print("    Significant difference detected!")
else:
    print("    No significant difference, models perform similarly.")

#  Save Best Models
joblib.dump(best_rf_model, "models/best_random_forest_tuned.pkl")
joblib.dump(best_xgb_model, "models/best_xgboost_tuned.pkl")

print("\n‚úÖ Hyperparameter Tuning Complete! Best models saved in `models/` folder.")


